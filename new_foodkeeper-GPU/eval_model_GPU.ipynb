{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3416d83",
   "metadata": {},
   "source": [
    "Here I am listing perhaps new resources as discovering new information regarding how to train spaCy has been quite complicated: \n",
    "\n",
    "Here we extract common terms within sections (ex. talking about technology) and gets the top x (where x is an an integer)\n",
    "https://towardsdatascience.com/improving-the-ner-model-with-patent-texts-spacy-prodigy-and-a-bit-of-magic-44c86282ea99\n",
    "\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "https://spacy.io/usage/training\n",
    "\n",
    "Current config file is set to \n",
    "\n",
    "Components: tagger, parser, ner\n",
    "Hardware: GPU (tranformer)\n",
    "optimized for: accuracy\n",
    "\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "More food objects that can be for use...\n",
    "https://fdc.nal.usda.gov/download-datasets.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e12205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.3.0/en_core_web_lg-3.3.0-py3-none-any.whl (400.7 MB)\n",
      "     -------------------------------------- 400.7/400.7 MB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (63.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ferna\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Libraries to use at your disposal, yaspin is just there so you can see that your code IS running,\n",
    "# \n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "# !pip3 install yaspin\n",
    "\n",
    "\n",
    "# When configured the config file, it added 'transformers', need to run this before we can move on...\n",
    "#!pip3 install spacy[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb45965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg\n",
    "'''\n",
    "The output: \n",
    "\n",
    "python -m spacy train config.cfg                   --paths.train ./train.spacy         --paths.dev ./dev.spacy\n",
    "\n",
    "From old model had this: \\\n",
    "                                 <DIFFERENT------->\n",
    "python -m spacy train config.cfg --output ./output --paths.train ./train.spacy         --paths.dev ./train.spacy'\n",
    "\n",
    "For new model perhaps...\n",
    "                                           <added>                <bit longer                                            <perhaps there for GPU>\n",
    "python -m spacy train config.cfg --output ./output --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b1cdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import re\n",
    "import os\n",
    "from spacy import displacy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "from pathlib import Path\n",
    "from yaspin import yaspin\n",
    "import random # randomizes training data\n",
    "from spacy.util import filter_spans\n",
    "import random # randomizes training data\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "FOODKEEPER_PATH = \"datasets/FoodKeeper-Data.xls\"\n",
    "TRAINING_DATA_PATH = \"datasets/data.csv\"\n",
    "# As I am creating a new model, I will leave the previous one untouched here\n",
    "MODEL_PATH = \"output/model-last\"\n",
    "TEST_DATA_PATH = \"datasets/test_data.csv\"\n",
    "REAL_TWITTER_DATA_PATH = \"datasets/data.csv\"\n",
    "#STARTING_KEYWORD_COUNT = 10\n",
    "#TRAINING_LOOP_ITERATIONS = 3\n",
    "#REQUIRED_KEYWORDS = 3\n",
    "# MODEL_PATH =  \"/dir_model\"\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "food_data = pd.read_excel(FOODKEEPER_PATH, sheet_name = \"Product\")\n",
    "\n",
    "## printing - bc |||EX: <col1> ID: 1 ~ Category_ID: 7 ~ Name: Butter ~ Name_subtitle: <blank> NaN ~ Keywords: Butter ~ ...\n",
    "# contains generic info such as the food item, keywords like name: cheese -cheese, parmesan, shredded, grated\n",
    "# there can be multiple of single name like cheese\n",
    "\n",
    "# contains patry info like it may be left at room temperature for 1-2 days < cheese>, time it takes for it to expire. refrigeration\n",
    "# tips and info, max time in there before it goes bad, freezer info like time in there, \n",
    "# print(\"PRINTING food_data excel sheet\")\n",
    "# print(food_data)\n",
    "# print(\"DONE ------------------------\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "training_data = pd.read_csv(TRAINING_DATA_PATH,index_col = False, header = None)\n",
    "\n",
    "\n",
    "training_data1 = training_data.sample(frac=0.7)\n",
    "training_data2 = training_data.drop(training_data1.index)\n",
    "\n",
    "#ex: A very special Friday evening....many will enjoy a relaxing glass of.....coffee! #lavathejavadog will \n",
    "#enjoy the remnants of a crunchie peanut butter jar. Life may throw stuff at us all....letâ€™s all eat peanut butter!\n",
    "#Enjoy your weekend! https://t.co/JME6ZmhZYN\n",
    "\n",
    "# print(\"PRINTING training_data excel sheet\")\n",
    "# print(training_data)\n",
    "# print(\"DONE ------------------------\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "## printing - bc\n",
    "# the tweet itself + food column indicating 1: there is a food mention within the tweet or 0: no food in the tweet\n",
    "# ex:just microwaved a kashi chicken and spinach thing, and put in the Milk dvd. anyone seen it? i bet it's good. i still havent seen slumdog\n",
    "# food: 1\n",
    "# print(\"PRINTING test_data excel sheet\")\n",
    "# print(test_data)\n",
    "# print(\"DONE ------------------------\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "live_tweets = pd.read_csv(REAL_TWITTER_DATA_PATH, header = None)\n",
    "\n",
    "## printing - bc contains unfiltered tweets\n",
    "\n",
    "#ex: A very special Friday evening....many will enjoy a relaxing glass of.....coffee! #lavathejavadog will \n",
    "#enjoy the remnants of a crunchie peanut butter jar. Life may throw stuff at us all....letâ€™s all eat peanut butter!\n",
    "#Enjoy your weekend! https://t.co/JME6ZmhZYN\n",
    "\n",
    "# print(\"PRINTING live_tweets excel sheet\")\n",
    "# print(live_tweets)\n",
    "# print(\"DONE ------------------------\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5596324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through and count the specific entities\n",
    "keywords = [] #'chicken', 'milk', 'butter', 'cheese'\n",
    "sampleData = []\n",
    "\n",
    "#\n",
    "#\n",
    "    \n",
    "#update rank tweet (based on amount of food entities) to take the counter as a parameter and condense both rankings\n",
    "def rankTweet(tweet):\n",
    "    model = spacy.load(MODEL_PATH)\n",
    "#     tweetKeywords = []\n",
    "    doc = model(tweet)\n",
    "    return len(doc.ents)\n",
    "       \n",
    "    \n",
    "# discovers new keywoords\n",
    "def findNewKeywords(tweet, keywords):\n",
    "    foodkeeperKeys = foodKeeperInfo() #Gathers all the keywords from the FoodKeeper database\n",
    "    x = tweet.split() #splits the tweet into a list\n",
    "    word = \"\" # current word that may be created\n",
    "    i = 0\n",
    "    # looping through the length of the list <word by word>\n",
    "    while i < len(x):\n",
    "    #for i in range(len(x)):\n",
    "        z = 1 # the \"jump\" to iterate\n",
    "        if x[i] in foodkeeperKeys:\n",
    "            word = x[i] # found a word in the database\n",
    "        try:\n",
    "            # if the combination of both words x + (x+1) matches a word in the database, make word = that word and increase z\n",
    "            # to 2 to make the correct iteration\n",
    "            foundBiWord = x[i] + \" \" + x[i+1]\n",
    "            if foundBiWord in foodkeeperKeys: #keywords\n",
    "                word = foundBiWord\n",
    "                z = 2\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            # try a triple combination of words after keyword x was found (x+1) + (x+2). if one found then word = that word and\n",
    "            # increase the counter by 3 to make correct iteration\n",
    "            foundTriWord = x[i] + \" \" + x[i+1] + \" \" + x[i+2]\n",
    "            if foundTriWord in foodkeeperKeys: #keywords:\n",
    "                word = foundTriWord\n",
    "                z = 3\n",
    "        except:\n",
    "            pass\n",
    "        i += z\n",
    "        \n",
    "        # add to the list of keywords (not where the foodKeeperInfo is <contains all keywords from database>)\n",
    "        # and returns it\n",
    "        if word not in keywords and word != \"\":\n",
    "            keywords.append(word)\n",
    "    return keywords\n",
    "    \n",
    "\n",
    "#Function to find the most common verbs in the tweets\n",
    "def getCommonVerbs(data):\n",
    "    import en_core_web_lg\n",
    "    # individual model here based on the library 'en_core_web_lg'\n",
    "    nlp2 = spacy.load(\"en_core_web_lg\")\n",
    "    count = 0\n",
    "    myVerbs = {}\n",
    "    # whats data here? \n",
    "    for i in range(len(data[0])):\n",
    "        doc = nlp2(data[0][i])\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\": # the part of speech, determine if the word itself is a verb like eat\n",
    "                if token.text in myVerbs: #token.text = word itself to see if it is already in the myVerbs\n",
    "                    myVerbs[token.text] = myVerbs[token.text] + 1 # increase the counter by 1 <counting number of times found?>\n",
    "                else:\n",
    "                    #...Default.stop_words are words like \"I\", \"the\", \"you\" which appear frequently but have no real meaning in the \n",
    "                    # spacy model. Thus it is not considered. If it is not that word, then it will be added to the verbs list with\n",
    "                    # a count of 1 <barely discovered once>\n",
    "                    if token.text not in nlp2.Defaults.stop_words:\n",
    "                        myVerbs[token.text] = 1\n",
    "        \n",
    "    # sorts the most occuring verbs? 10 of them only?\n",
    "    topVerbs = dict(sorted(myVerbs.items(), key = lambda item: item[1], reverse=True)[:10])\n",
    "    return [key for key in topVerbs]\n",
    "    \n",
    "\n",
    " #\n",
    "#\n",
    "# Converts a tweet into a training format with each post going through a process of finding keywords and appending entities\n",
    "#\n",
    "#\n",
    "def convertToTrainingFormat(tweet, keywords):\n",
    "    foodKeeperKeywordsTest = foodKeeperInfo() #Gathers all the keywords from the FoodKeeper database\n",
    "    x = tweet.split()\n",
    "    myEnts = {'entities':[]}\n",
    "    found = False\n",
    "    i = 0\n",
    "    foundWords = []\n",
    "    while i < len(x):\n",
    "        z = 1\n",
    "        newWord = \"\"\n",
    "        if x[i] in keywords:\n",
    "            pos = tweet.find(x[i]) # gets the index of where we found the word in the actual tweet\n",
    "            # (index of where word was found, pos + length of word = end index of the word in tweet, set as a food object <?>)\n",
    "            # set found to true -> found a word\n",
    "            y = (pos, pos + len(x[i]), 'FOOD')\n",
    "            found = True\n",
    "        if x[i] in foodKeeperKeywordsTest:\n",
    "            newWord = x[i]\n",
    "            \n",
    "        try:\n",
    "            # try same thing as above but with 2 words\n",
    "            # ex if we saw something like \"sour\", in the if before it would not locate it. Here we would locate \"sour X\" such as\n",
    "            # \"sour cream\" without having to trace backwards\n",
    "            foundBiWord = x[i] + \" \" + x[i+1]\n",
    "            if foundBiWord in keywords:\n",
    "                pos = tweet.find(x[i])\n",
    "                y = (pos, pos + len(x[i])+len(x[i+1]) + 1, 'FOOD')\n",
    "                found = True\n",
    "                z = 2\n",
    "            if foundBiWord in foodKeeperKeywordsTest:\n",
    "                newWord = foundBiWord\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            # try same thing as with 1 word but with 3 words\n",
    "            foundTriWord = x[i] + \" \" + x[i+1] + \" \" + x[i+2]\n",
    "            if foundTriWord in keywords:\n",
    "                pos = tweet.find(x[i])\n",
    "                y = (pos, pos + len(x[i])+len(x[i+1])+len(x[i+2]) + 2, 'FOOD')\n",
    "                found = True\n",
    "                z = 3\n",
    "            if foundTriWord in foodKeeperKeywordsTest:\n",
    "                newWord = foundTriWord\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #my own shot for experimental views\n",
    "        try:\n",
    "            #4 word attempt\n",
    "            found_quad_word = x[i] + \" \" + x[i + 1] + \" \" + x[i + 2] + \" \" + x[i + 3]\n",
    "            if found_quad_word in keywords:\n",
    "                pos = tweet.find(x[i])\n",
    "                y = (pos, pos + len(x[i]) + len(x[i + 1]) + len(x[i + 2]) + len(x[i + 3]) + 3, 'FOOD')\n",
    "                found = True\n",
    "                z = 4\n",
    "            if found_quad_word in foodKeeperKeywoordsTest:\n",
    "                newWord = found_quad_word\n",
    "        except:\n",
    "            pass\n",
    "        # end of trial for my owns sake\n",
    "        \n",
    "        # if that word is not currently as an entity, append the word into that list in the dictionary, otherwise continue\n",
    "        try:\n",
    "            if y not in myEnts['entities']:\n",
    "                myEnts['entities'].append(y)    \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # declared lines below <if the work is not within the keywordRanker, create a spot for it and set it to counter 1\n",
    "        #elif increment the counter <meaning we found a word that was already within the keywordRanker>\n",
    "        if newWord != \"\" and newWord not in keywordRanker:\n",
    "            keywordRanker[newWord] = 1\n",
    "        elif newWord != \"\" and newWord in keywordRanker:\n",
    "            keywordRanker[newWord] += 1\n",
    "        #print(z)\n",
    "        i += z\n",
    "        #print(i)\n",
    "    \n",
    "    # format the tweet alongside with the entities found within the tweet\n",
    "    formatted = (tweet, myEnts)\n",
    "    #print(formatted)\n",
    "    # means we did find a related word towards the tweet, otherwise return an empty tuple (i think)\n",
    "    if found:\n",
    "        return formatted\n",
    "    else: \n",
    "        return ()\n",
    "     \n",
    "# food_data contains patry info like it may be left at room temperature for 1-2 days < cheese>, time it takes for it to expire. refrigeration\n",
    "# tips and info, max time in there before it goes bad, freezer info like time in there,\n",
    "\n",
    "def foodKeeperInfo():              \n",
    "    keywords = []\n",
    "    for word in food_data['Name']: #Butter, Buttermilk, Cheese,...\n",
    "        word = word.replace(\" or \", \" \") #word or to \" \"\n",
    "\n",
    "        # print(\"before re.sub[,..]: \" + word)\n",
    "        # word = re.sub('[/]', ' ', word) # sub a character to a ' '\n",
    "        # print(\"after: \" + word)\n",
    "        split_word = word.split(\",\")\n",
    "        word = re.sub('[/,]', ' ', word) # sub a character to a ' '\n",
    "        if len(split_word) > 1:\n",
    "            # still appending as before just in case ex. \"shellfish, oysters\" -> \"shellfish  oysters\"\n",
    "            word = word.lstrip() # rid of left hand side empty characters\n",
    "            word = word.rstrip() # rid of right hand side empty characters\n",
    "\n",
    "#             if word.lower() not in keywords: \n",
    "#                 keywords.append(word.lower())\n",
    "\n",
    "            for x in split_word:\n",
    "                # words like 'butter', 'cheese', would fall into this category\n",
    "                if len(x.split()) == 1:\n",
    "                    temp_word = x.lstrip()\n",
    "                    temp_word = temp_word.rstrip()\n",
    "\n",
    "                    if temp_word.lower() not in keywords:\n",
    "                        keywords.append(temp_word.lower()) \n",
    "                # words such as 'Cheese,cheddar, swiss,parmesan' split by ',' will be seperated as they would very rarely be found\n",
    "                # attributed all as a single entity.\n",
    "                else:\n",
    "                    temp_word = x.lstrip()\n",
    "                    temp_word = temp_word.rstrip()\n",
    "\n",
    "                    var = temp_word.split()\n",
    "                    # it may be as 'Cheese,cheddar, swiss, and parmesan' which may cause problems as itself is not a FOOD, will be\n",
    "                    # ignored\n",
    "                    if var[0].lower() == \"and\":\n",
    "                        temp_word = temp_word[3:]\n",
    "                        temp_word = temp_word.lstrip()\n",
    "                    \n",
    "                        \n",
    "                    if temp_word.lower() not in keywords:\n",
    "                        keywords.append(temp_word.lower()) \n",
    "\n",
    "        else:    \n",
    "            word = word.lstrip() # rid of left hand side empty characters\n",
    "            word = word.rstrip() # rid of right hand side empty characters\n",
    "\n",
    "            # lower cases all the word <if it isn't in keywords> and appends them to the keywords list\n",
    "            if word.lower() not in keywords: \n",
    "                keywords.append(word.lower())\n",
    "\n",
    "    #print(\"Total foodkeeper food names: \" + str(len(keywords)))        \n",
    "    #for element in sorted(keywords):\n",
    "        #print(element)\n",
    "        \n",
    "    # return the keywords witih the food_data set into a list formatted accordingly\n",
    "    return keywords\n",
    "\n",
    "\n",
    "foodKeeperKeywordsTest = foodKeeperInfo()\n",
    "\n",
    "# pre-processing of a tweet (can be looked upon deeper such as changing puncuation, though it did not work for me too well).\n",
    "def preProcess(tweet):\n",
    "    #Converts a tweet to lowercase, replaces anyusername w/ <USERNAME> and URLS with <URL>\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('@[a-zA-z0-9]*', '', tweet)              # <USERNAME>\n",
    "    tweet = re.sub('http[a-zA-z0-9./:]*', '', tweet)       # <URL>\n",
    "    # currently gets rid of ',' and '.' , but not '!', nor '?'.\n",
    "    # with my changes leave the puncuation of ',' and '.' inside, \n",
    "    # results without filtering ',.!?' dropped 88->82\n",
    "    # now testing filtering ALL punctuation\n",
    "#     tweet = re.sub('!?,;', '.', tweet)\n",
    "\n",
    "    # tweet = re.sub('[:;.,-?!]*', '', tweet) #going back to the original with more words (25) + entity counter 2 + get rid of ! and ?\n",
    "\n",
    "    # original\n",
    "    tweet = re.sub('[.,-]*', '', tweet)\n",
    "    \n",
    "    tweet = re.sub('&amp;', 'and', tweet) # whats &amp;?\n",
    "    \n",
    "    return tweet\n",
    "#\n",
    "#\n",
    "# wats this\n",
    "noEntity= []\n",
    "\n",
    "keywordRanker = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8285d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find out what data explicitly means (was not called elsewhere? to what I know - BC)\n",
    "def information(data):\n",
    "    myData = {}\n",
    "    totalEnt=0\n",
    "    # believe data at data[0] is tweets\n",
    "    for i in range(len(data[0])):\n",
    "        #extracts each tweet <processes it as mentioned below> and puts it in the nlp->doc\n",
    "        # preProcess #Converts a tweet to lowercase, replaces anyusername w/ <USERNAME> and URLS with <URL>\n",
    "        doc = nlp(preProcess(data[0][i]))\n",
    "        if len(doc.ents) == 0:\n",
    "            noEntity.append(preProcess(data[0][i]))\n",
    "            \n",
    "        # bottom line was commented out but using it to identity what the length is and what doc.ent contains\n",
    "        #print(len(doc.ents))\n",
    "        #print(doc.ents)\n",
    "        if(len(doc.ents) == 4):\n",
    "            print(doc)\n",
    "            \n",
    "        for entity in doc.ents: \n",
    "        #print(entity.label_)\n",
    "            totalEnt+=1\n",
    "            if(entity.label_ == 'FOOD'):\n",
    "                if entity.text in myData:\n",
    "                    myData[entity.text] += 1\n",
    "                else:\n",
    "                    myData[entity.text] = 1\n",
    "                    \n",
    "    print(\"Number of entities found: \" + str(len(myData)))\n",
    "    print(totalEnt)\n",
    "    for i in sorted(myData, key = myData.get):\n",
    "        print(\"Entity: \" + i, \"Count: \" + str(myData[i]), \"Density: \" + str(format(myData[i]/totalEnt, '.2f')), end = \"\\n\")\n",
    "    \n",
    "    \n",
    "    return myData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(test_data['tweet'])):\n",
    "    test_data['tweet'][i] = preProcess(test_data['tweet'][i])\n",
    "\n",
    "y = test_data['food'].tolist()\n",
    "# print(test_data)\n",
    "# print(nlp.pipe_names)\n",
    "    \n",
    "# renders with displacy an entity (food)\n",
    "def ent_recognize(text):\n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    doc = nlp(text)\n",
    "    displacy.render(doc,style = \"ent\")\n",
    "    \n",
    "# renders with displacy if an entity was discovered (food)\n",
    "def predict(tweet):\n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    doc = nlp(str(tweet))\n",
    "    if doc.ents:\n",
    "        displacy.render(doc,style = \"ent\")\n",
    "\n",
    "# returns a boolean prediction if a food was discovered (Look upon if we should add a more explicit one that counts amount of keywords discovered)\n",
    "def returnPrediction(tweet):   \n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    doc = nlp(str(tweet))\n",
    "    if doc.ents:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Calls returnPredictions(tweet) for each individual tweet in the test_data dataset\n",
    "def get_predictions():\n",
    "    predictions = []\n",
    "    for tweet in test_data['tweet'].tolist():\n",
    "        predictions.append(returnPrediction(tweet))\n",
    "    return predictions\n",
    "\n",
    "# evaluations current model by getting current predictions. \n",
    "def eval_model(model):\n",
    "    nlp = spacy.load(model)\n",
    "    predictions = get_predictions()\n",
    "    print(metrics.confusion_matrix(y,predictions, labels = [1,0]))\n",
    "    print(metrics.classification_report(y,predictions, labels = [1,0]))\n",
    "    \n",
    "    \n",
    "def eval_model():\n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    predictions = get_predictions()\n",
    "    print(metrics.confusion_matrix(y,predictions, labels = [1,0]))\n",
    "    print(metrics.classification_report(y,predictions, labels = [1,0]))\n",
    "    \n",
    "def show_tp():\n",
    "    counter = 0\n",
    "    tweets = test_data['tweet'].tolist()\n",
    "    predictions = get_predictions()\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 1 and y[i] == 1:\n",
    "            print(\"True positives:\", tweets[i], \"\\n\")\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "def show_tn():\n",
    "    counter = 0\n",
    "    predictions = get_predictions()\n",
    "    tweets = test_data['tweet'].tolist()\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 0 and y[i] == 0:\n",
    "            print(\"True Negative:\", tweets[i], \"\\n\")\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "def show_fn():\n",
    "    predictions = get_predictions()\n",
    "    tweets = test_data['tweet']\n",
    "    counter = 0\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 0 and y[i] == 1:\n",
    "            print(\"False Negative:\", tweets[i], \"\\n\")\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "def show_fp():\n",
    "    predictions = get_predictions()\n",
    "    tweets = test_data['tweet'].tolist()\n",
    "    for i in range(len(y)):\n",
    "        if predictions[i] == 1 and y[i] == 0:\n",
    "            print(\"False Positive:\")\n",
    "            doc = nlp(str(tweets[i]))\n",
    "            if doc.ents:\n",
    "                displacy.render(doc,style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df37c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to the directory given\n",
    "def saving(model, directory):\n",
    "    out_dir = Path(directory)\n",
    "    if not out_dir.exists():\n",
    "        print(\"Directory '\" + str(directory) + \"' not found! Creating the directory to save model...\")\n",
    "        out_dir.mkdir()\n",
    "    \n",
    "    print(\"writing to \", str(out))\n",
    "    model.to_disk(out_dir)\n",
    "    \n",
    "\n",
    "def load(directory):\n",
    "    try:\n",
    "        print(\"successfully loaded from directory\")\n",
    "        model = spacy.load(directory)\n",
    "        return model\n",
    "    except:\n",
    "        print(\"No model within the given directory...\")\n",
    "        return\n",
    "\n",
    "\n",
    "def create(model):\n",
    "    if (model is not None):\n",
    "        print(\"loading model...\")\n",
    "        model = spacy.load(model)\n",
    "    else:\n",
    "        print(\"No model was found... creating a 'en_core_web_sm' model...\")\n",
    "        model = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14ac76",
   "metadata": {},
   "source": [
    "## Abandoned Model with different approach.\n",
    "Note that this one was explicitly trying to train directly to spaCy with the given code. However, it was not working as desired. It was left here as scrap code (along with the block of code above with create(), saving(), ect.) if perhaps it will be to any use. For reference, it was following a similar path to this one. https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/. Note that this was not a direct application towards ours and there were needed changes. Perhaps there are more chagnes there still needed to get this one to work. The current model would be the next function (not training_new(...)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c38ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new spacy training model\n",
    "def training_new(data, model, commonVerbs):\n",
    "    from spacy.training.example import Example\n",
    "    \n",
    "    keywords = [] #foodKeeperInfo()\n",
    "    oldKeywords = []\n",
    "    newKeywords = []\n",
    "    # how many times we plan to iterate\n",
    "    ITER = 12\n",
    "    FOOD = \"FOOD\"\n",
    "    \n",
    "    #entityCheckCount controls how many entities are required to \n",
    "    #add a Tweet to be trained\n",
    "    \n",
    "    # at first it will be 'en_core_web_sm' if it is None it will create the same type anyways...\n",
    "    nlp = create(model)\n",
    "    # getting the NER component \n",
    "    ner = nlp.get_pipe('ner')\n",
    "    ner.add_label(FOOD)\n",
    "    nlp.meta['name'] = 'food_ner'\n",
    "\n",
    "\n",
    "\n",
    "    #Loop through all the tweets\n",
    "    #This loop is necessary to get the most common keywords \n",
    "    #in the convertToTrainingFormat function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     # to train now that we have appropiate data\n",
    "    optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "\n",
    "    # to only train the current pipes (can be changed)\n",
    "    pipes = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    # pipes we do not want to train\n",
    "    no_pipes = [pipe for pipe in nlp.pipe_names if not pipe not in pipes]\n",
    "    \n",
    "    \n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    with nlp.disable_pipes(*no_pipes) :\n",
    "        \n",
    "        # Have to see what the arguments within compounding specifically are about...\n",
    "        sizes = compounding (5.0, 50.0, 1.001) \n",
    "        db = DocBin() # create a DocBin object\n",
    "        myTweets = []\n",
    "        entityCheckCount = 3\n",
    "        #Loop through all the tweets\n",
    "        #This loop is necessary to get the most common keywords \n",
    "        #in the convertToTrainingFormat function\n",
    "        counter = 0\n",
    "        trainingLoop = True\n",
    "\n",
    "        while trainingLoop:\n",
    "            with yaspin() as loading:     \n",
    "\n",
    "                # getting the training data\n",
    "                loading.text = \"Gathering proper training data on iteration \" + str(counter + 1)\n",
    "                for i in range(len(data[0])): #len(data[0])\n",
    "\n",
    "                    if counter == 0:\n",
    "                        x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                        # x = (tweet, myEnts)\n",
    "                    #If counter is 1 then there is no model to check so \n",
    "                    #a word count is performed\n",
    "                    elif counter == 1:\n",
    "                        x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                        if x!= ():\n",
    "                            if len(x[1]['entities']) > entityCheckCount:\n",
    "                                #print(\"Found tweet\", x[0])\n",
    "                                myTweets.append(x)  \n",
    "                    else:\n",
    "                        #Convert each tweet into spacy training format\n",
    "                        x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                        checkPassed = False\n",
    "                        if x != ():\n",
    "                            #Check the ranking of the tweet\n",
    "\n",
    "# nlp was model (maybe will have to change to model)\n",
    "                            # checks the rank with the model it loaded (previous one or the current one that is being trained?)\n",
    "                            if rankTweet(x[0]) > entityCheckCount:\n",
    "                                checkTweet = x[0].split()\n",
    "\n",
    "                                #Check to see if tweet has one of the common verbs\n",
    "                                for word in checkTweet:\n",
    "                                    if word in commonVerbs:\n",
    "                                        checkPassed = True\n",
    "\n",
    "                                if True: #checkPassed:\n",
    "                                    #print(\"Checking rank...\")\n",
    "                                    myTweets.append(x)\n",
    "\n",
    "                loading.text = \"Now training on iteration \" + str(counter + 1)               \n",
    "                random.shuffle(myTweets)\n",
    "                \n",
    "                if counter == 0:\n",
    "                    sortedKeywords =  sorted(keywordRanker, key=keywordRanker.get, reverse=True)\n",
    "                    \n",
    "                    # current amount of keywords (food entities) known\n",
    "                    for i in range(15):\n",
    "                        keywords.append(sortedKeywords[i])\n",
    "                \n",
    "                # actual training occurring here...\n",
    "                elif counter > 0 and counter < 3:\n",
    "                    # since there is not as much data in lower iterations of the general training scheme,\n",
    "                    # make it go through this data more\n",
    "                    for i in range(20):\n",
    "                        # shuffle it up\n",
    "                        random.shuffle(myTweets)\n",
    "                        losses = {}\n",
    "                        \n",
    "#                         for raw_text, entity_offsets in MyTweets:\n",
    "#                             doc = nlp.make_doc(raw_text)\n",
    "#                             example = Example.from_\n",
    "                        \n",
    "                        # batches the data \n",
    "                        # Have to see what the arguments within compounding specifically are about...\n",
    "                        #sizes = compounding (5.0, 50.0, 1.001) \n",
    "                        #batches = minibatch(myTweets, size=2)\n",
    "                        \n",
    "                        for batch in minibatch(myTweets, size=2):\n",
    "                            \n",
    "                            for text, annotations in batch:\n",
    "                                #text, annot = zip(*group)\n",
    "\n",
    "                                doc = nlp.make_doc(text)\n",
    "                                example = Example.from_dict(doc, annotations)\n",
    "                                # batch of texts\n",
    "                                # batch of annotations\n",
    "                                # update the model with gathered data\n",
    "                                # dropout - make it harder to memorise data\n",
    "                                nlp.update([example], sgd=optimizer, drop=0.35)\n",
    "\n",
    "                \n",
    "                elif counter > 3:\n",
    "                    \n",
    "                    # have it go through the same data (shuffled) with 2 iterations?\n",
    "                    \n",
    "                    for i in range(2):\n",
    "                        # shuffle it up\n",
    "                        random.shuffle(myTweets)\n",
    "                        losses = {}\n",
    "                        # batches the data \n",
    "                        #sizes = compounding (5.0, 50.0, 1.001)\n",
    "                        batches = minibatch(myTweets, size=2)\n",
    "\n",
    "\n",
    "                        for batch in minibatch(myTweets, size=2):\n",
    "                            \n",
    "                            for text, annotations in batch:\n",
    "                                #text, annot = zip(*group)\n",
    "\n",
    "                                doc = nlp.make_doc(text)\n",
    "                                example = Example.from_dict(doc, annotations)\n",
    "                                # batch of texts\n",
    "                                # batch of annotations\n",
    "                                # update the model with gathered data\n",
    "                                # dropout - make it harder to memorise data\n",
    "                                nlp.update([example], sgd=optimizer, drop=0.35)\n",
    "\n",
    "                        \n",
    "                # saving the model\n",
    "                loading.text = \"Saving the model on iteration \" + str(counter + 1)\n",
    "                saving(nlp, \"/dir_model/\")\n",
    "                \n",
    "                \n",
    "                print(\"Total keywords: \", str(len(keywords)))\n",
    "                print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "                oldKeywords = len(keywords)\n",
    "            \n",
    "                #Look for new keywords\n",
    "                for element in myTweets:\n",
    "                    keywords = findNewKeywords(element[0], keywords)\n",
    "                \n",
    "            \n",
    "                #No new keywords are found\n",
    "                if (oldKeywords == len(keywords)) and counter > 1 and entityCheckCount != 1:\n",
    "                    entityCheckCount -= 1\n",
    "                    print(\"Decreasing entityCheckCount variable by 1\")\n",
    "                    print(\"entityCheckCount = \", entityCheckCount)\n",
    "                \n",
    "\n",
    "                #New keywords are found and entity rank check == 1\n",
    "                elif (oldKeywords == len(keywords)) and entityCheckCount == 1:\n",
    "                    trainingLoop = False\n",
    "            \n",
    "                eval_model()\n",
    "                \n",
    "                print(\"Total keywords: \", str(len(keywords)))\n",
    "                print(\"Total Tweets: \", str(len(myTweets)))\n",
    "                print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "                counter += 1\n",
    "                \n",
    "    print(\"Training is completed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2cfc3e",
   "metadata": {},
   "source": [
    "## Current Model GPU\n",
    "This is the current model, it runs on a GPU. If the gpu model is not applicable to you, make sure to use the CPU one instead or find other needs of running it. The main gist of this one is where we write to disk. The main game changer would be to look at the config file for the spaCy model. Elsewhere, much changes cannot be made. Though there is the option of trying a 70/30 split with the current TRAINING dataset to attempt with the 30% dataset testing. \n",
    "\n",
    "Note that the main thing to look at would be the config file. More documentation can be looked upon here: https://spacy.io/usage/training#config-custom\n",
    "If possible, change the config file, look into efficiency for quicker runtime. Currently, the config file is set to accuracy -> long runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01b32206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old model\n",
    "def trainModel(data, commonVerbs):\n",
    "    #Initialize all the variables\n",
    "    keywords = [] #foodKeeperInfo()\n",
    "    oldKeywords = []\n",
    "    newKeywords = []\n",
    "    \n",
    "    print(\"Getting the common verbs...\")\n",
    "    print(\"Common Verbs gathered...\", '\\n')\n",
    "\n",
    "    print(commonVerbs)\n",
    "    \n",
    "    #entityCheckCount controls how many entities are required to \n",
    "    #add a Tweet to be trained\n",
    "    \n",
    "    entityCheckCount = 3\n",
    "    \n",
    "    counter = 0\n",
    "    trainingLoop = True\n",
    "    with yaspin() as loading:\n",
    "        while trainingLoop:\n",
    "            loading.text = \"Starting training process\"\n",
    "            counterText = \"~~~~~~~~~~~~~~~~~\"+str(counter)+\"~~~~~~~~~~~~~~~~~\"\n",
    "            print(counterText)\n",
    "\n",
    "            nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "            try:\n",
    "                # if we haven't trained anything with a previously trained model. Nothing will load.\n",
    "                print(\"getting from path: \", str(MODEL_PATH))\n",
    "                model = spacy.load(MODEL_PATH)\n",
    "                print('Model loaded...')\n",
    "            except:\n",
    "                print('No model...')\n",
    "\n",
    "            db = DocBin() # create a DocBin object\n",
    "\n",
    "            myTweets = []\n",
    "\n",
    "            #Loop through all the tweets\n",
    "            #This loop is necessary to get the most common keywords \n",
    "            #in the convertToTrainingFormat function\n",
    "\n",
    "\n",
    "            # getting the training data \n",
    "            loading.text = \"Gathering training data...\"\n",
    "            for i in range(len(data[0])): #len(data[0])\n",
    "                #useless if?\n",
    "    #             if i % 500 == 0:\n",
    "    #                 print(i)\n",
    "\n",
    "                if counter == 0:\n",
    "                    x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                    # x = (tweet, myEnts)\n",
    "                #If counter is 1 then there is no model to check so \n",
    "                #a word count is performed\n",
    "                elif counter == 1:\n",
    "                    x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                    if x!= ():\n",
    "                        if len(x[1]['entities']) > entityCheckCount:\n",
    "                            #print(\"Found tweet\", x[0])\n",
    "                            myTweets.append(x)  \n",
    "                else:\n",
    "                    #Convert each tweet into spacy training format\n",
    "                    x = convertToTrainingFormat(preProcess(data[0][i]), keywords)\n",
    "                    checkPassed = False\n",
    "                    if x != ():\n",
    "                        #Check the ranking of the tweet\n",
    "\n",
    "                        # checks the rank with the model it loaded (previous one or the current one that is being trained?)\n",
    "                        if rankTweet(x[0]) > entityCheckCount:\n",
    "                            checkTweet = x[0].split()\n",
    "\n",
    "                            #Check to see if tweet has one of the common verbs\n",
    "                            for word in checkTweet:\n",
    "                                if word in commonVerbs:\n",
    "                                    checkPassed = True\n",
    "\n",
    "                            if True: #checkPassed:\n",
    "                                #print(\"Checking rank...\")\n",
    "                                myTweets.append(x)\n",
    "\n",
    "                ######\n",
    "                ###### ^ formatted all the tweets with tweet and entities\n",
    "                ######\n",
    "\n",
    "\n",
    "\n",
    "    #         Initialize the keywords\n",
    "\n",
    "    #         # format the tweet alongside with the entities found within the tweet\n",
    "    #     formatted = (tweet, myEnts)\n",
    "    #       Value of the myTweets \n",
    "    \n",
    "            loading.text = \"sorting data for docBin file and training if needed on iteration: \" + str(counter)\n",
    "            if counter == 0:  \n",
    "                # Set keywords to be all keywords found in foodkeeper\n",
    "    #            keywords = foodKeeperInfo()\n",
    "                sortedKeywords =  sorted(keywordRanker, key=keywordRanker.get, reverse=True)\n",
    "\n",
    "                # changing the range of (15) to 30  # now to 22\n",
    "                for i in range(15): #sortedKeywords\n",
    "                    keywords.append(sortedKeywords[i])\n",
    "                #print(sortedKeywords[i], keywordRanker[sortedKeywords[i]])\n",
    "\n",
    "\n",
    "            elif counter > 0:\n",
    "                for text, annot in tqdm(myTweets): # data in previous format\n",
    "                    doc = nlp.make_doc(text) # create doc object from text\n",
    "                    ents = []\n",
    "\n",
    "\n",
    "\n",
    "                    # start index of food entity, end index, label = Food, alignment =  (span of all tokens completely within the character span)\n",
    "                    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "\n",
    "                        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "\n",
    "                        # none when the character indices dont map to a valid span\n",
    "                        if span is None:\n",
    "                            print(\"Skipping entity\")\n",
    "                        elif ents == []: ents.append(span) \n",
    "                        else:\n",
    "                            #Check to see if any entities are overlapping i.e rice and rice cakes\n",
    "                            ents.append(span)\n",
    "\n",
    "\n",
    "    #                         for ent in ents:\n",
    "    #                             if ent is not None:\n",
    "    #                                 entLength = ent.end - ent.start\n",
    "    #                                 if span.start == ent.start or span.end == ent.end:\n",
    "    #                                     if entLength > (span.end - span.start):\n",
    "    #                                         continue\n",
    "    #                                     else:\n",
    "    #                                         ents.remove(ent)\n",
    "    #                                         ents.append(span)\n",
    "    #                                 else:\n",
    "    #                                     ents.append(span)\n",
    "                        #if span not in ents: ents.append(span)\n",
    "\n",
    "                                    #print(span.start, span.end, ents)\n",
    "                        #put into for loop\n",
    "    #                 foundEnts = []\n",
    "    #                 newEnts = []\n",
    "    #                 for ent in ents:\n",
    "    #                     if ent.text not in foundEnts:\n",
    "    #                         foundEnts.append(ent.text)\n",
    "    #                         newEnts.append(ent)\n",
    "                    newEnts = filter_spans(ents)              \n",
    "                    #try:\n",
    "                    doc.ents = newEnts # label the text with the ents\n",
    "                        #print(doc)\n",
    "                    db.add(doc)\n",
    "                    #except:\n",
    "                        #print(\"Error 10: \", doc)\n",
    "\n",
    "\n",
    "\n",
    "    # Original code that wrote to the disk                      \n",
    "    #-----------------------------------------------------------------------------------\n",
    "                # used to be inside (\"./training.spacy\") (without the _data)\n",
    "                db.to_disk(\".train_data.spacy\") # save the docbin object\n",
    "\n",
    "                #If problems are occuring with the models not appearing\n",
    "                #ensure that the command is valid, specifically python is the correct\n",
    "                #PATH variable name on your machine\n",
    "    # Original code that wrote to the disk           \n",
    "    # ----------------------------------------------------------------------------------------------------------------------\n",
    "                #--paths.train should be where the docbin object is saved\n",
    "                stream = os.popen('python -m spacy train config.cfg --output ./output --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0')\n",
    "                print(stream.read())\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------\n",
    "                loading.text = \"Done with current training on iteration: \" + str(counter)\n",
    "                print(\"Total keywords: \", str(len(keywords)))\n",
    "                print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "                oldKeywords = len(keywords)\n",
    "\n",
    "\n",
    "                #Look for new keywords\n",
    "                for element in myTweets:\n",
    "                    keywords = findNewKeywords(element[0], keywords)\n",
    "\n",
    "\n",
    "                #No new keywords are found\n",
    "                if (oldKeywords == len(keywords)) and counter > 1 and entityCheckCount != 1:\n",
    "                    entityCheckCount -= 1\n",
    "                    print(\"Decreasing entityCheckCount variable by 1\")\n",
    "                    print(\"entityCheckCount = \", entityCheckCount)\n",
    "\n",
    "\n",
    "                #New keywords are found and entity rank check == 1\n",
    "                elif (oldKeywords == len(keywords)) and entityCheckCount == 1:\n",
    "                    trainingLoop = False\n",
    "\n",
    "                eval_model()\n",
    "\n",
    "\n",
    "\n",
    "            #for element in myTweets:\n",
    "                #findNewKeywords(element[0], keywords)\n",
    "\n",
    "            print(\"Total keywords: \", str(len(keywords)))\n",
    "            print(\"Total Tweets: \", str(len(myTweets)))\n",
    "            print(\"List of Keywords:\\n\\n\",keywords,\"\\n\\n\")\n",
    "            counter += 1\n",
    "        \n",
    "    print('Training Done...')\n",
    "        \n",
    "        # spinach < not in dataset> \n",
    "        # i like to eat spinach <known to have a keyword\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1feac96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['butter',\n",
       " 'buttermilk',\n",
       " 'cheese',\n",
       " 'coffee creamer',\n",
       " 'cottage cheese',\n",
       " 'cream cheese',\n",
       " 'cream',\n",
       " 'dips',\n",
       " 'egg substitutes',\n",
       " 'eggnog',\n",
       " 'eggs',\n",
       " 'egg dishes',\n",
       " 'kefir',\n",
       " 'margarine',\n",
       " 'milk',\n",
       " 'pudding',\n",
       " 'sour cream',\n",
       " 'whipped cream',\n",
       " 'whipped topping',\n",
       " 'yogurt',\n",
       " 'beef',\n",
       " 'lamb',\n",
       " 'veal',\n",
       " 'pork',\n",
       " 'goat',\n",
       " 'venison',\n",
       " 'variety meats',\n",
       " 'bacon',\n",
       " 'corned beef',\n",
       " 'ham',\n",
       " 'hot dogs',\n",
       " 'sausage',\n",
       " 'stuffed',\n",
       " 'raw pork chops',\n",
       " 'raw kabobs with vegetables',\n",
       " 'jerky',\n",
       " 'meat products',\n",
       " 'retort pouches boxes',\n",
       " 'chicken',\n",
       " 'turkey',\n",
       " 'ground turkey chicken',\n",
       " 'chicken parts',\n",
       " 'turkey parts',\n",
       " 'duckling',\n",
       " 'goose',\n",
       " 'pheasant',\n",
       " 'quail',\n",
       " 'capon',\n",
       " 'cornish hens',\n",
       " 'giblets',\n",
       " 'raw chicken breasts',\n",
       " 'turducken',\n",
       " 'chicken nuggets',\n",
       " 'patties',\n",
       " 'cooked poultry dishes',\n",
       " 'fried chicken',\n",
       " 'poultry pieces',\n",
       " 'rotisserie chicken',\n",
       " 'canned chicken',\n",
       " 'lean fish',\n",
       " 'fatty fish',\n",
       " 'caviar',\n",
       " 'cooked fish',\n",
       " 'surimi seafood',\n",
       " 'scallops',\n",
       " 'shrimp',\n",
       " 'crayfish',\n",
       " 'squid',\n",
       " 'shucked clams',\n",
       " 'mussels',\n",
       " 'oysters',\n",
       " 'crab meat',\n",
       " 'crab legs',\n",
       " 'live clams',\n",
       " 'crab',\n",
       " 'fresh whole lobster',\n",
       " 'fresh lobster tails',\n",
       " 'fresh clams',\n",
       " 'cooked shellfish',\n",
       " 'herring',\n",
       " 'fish',\n",
       " 'tofu',\n",
       " 'miso',\n",
       " 'soy flour',\n",
       " 'textured soy protein',\n",
       " 're-hydrated textured soy protein',\n",
       " 'leftovers',\n",
       " 'commercial brand vacuum-packed dinners',\n",
       " 'cooked pasta',\n",
       " 'cooked rice',\n",
       " 'fruit',\n",
       " 'cut',\n",
       " 'guacamole',\n",
       " 'hummus',\n",
       " 'luncheon meat poultry',\n",
       " 'main dishes meals',\n",
       " 'meats',\n",
       " 'olives',\n",
       " 'pate',\n",
       " 'soup',\n",
       " 'stews',\n",
       " 'casseroles',\n",
       " 'commercial bread products',\n",
       " 'tortillas',\n",
       " 'commercial cakes and muffins',\n",
       " 'cheesecake',\n",
       " 'cookies',\n",
       " 'dairy filled eclairs',\n",
       " 'doughnuts',\n",
       " 'fruit cake',\n",
       " 'pastries',\n",
       " 'danish',\n",
       " 'cream pies',\n",
       " 'chiffon pies',\n",
       " 'fruit pies',\n",
       " 'pies',\n",
       " 'quiche',\n",
       " 'baking powder',\n",
       " 'baking soda',\n",
       " 'biscuit pancake mix',\n",
       " 'cake',\n",
       " 'brownie',\n",
       " 'bread mixes',\n",
       " 'chocolate',\n",
       " 'cocoa and cocoa mixes',\n",
       " 'cornmeal',\n",
       " 'cornstarch',\n",
       " 'flour',\n",
       " 'frosting icing',\n",
       " 'gelatin',\n",
       " 'oils',\n",
       " 'nut oils',\n",
       " 'vegetable oil sprays',\n",
       " 'shortening',\n",
       " 'tamarind paste',\n",
       " 'chili powder',\n",
       " 'seasoning blends',\n",
       " 'flavored herb mixes',\n",
       " 'garlic',\n",
       " 'herbs',\n",
       " 'spice spices',\n",
       " 'sugar',\n",
       " 'sugar substitutes',\n",
       " 'tapiocas',\n",
       " 'tube cans',\n",
       " 'ready-to-bake pie crust',\n",
       " 'cookie dough',\n",
       " 'apples',\n",
       " 'apricots',\n",
       " 'avocados',\n",
       " 'bananas',\n",
       " 'berries',\n",
       " 'blueberries',\n",
       " 'cherimoya',\n",
       " 'citrus fruit',\n",
       " 'coconut',\n",
       " 'coconuts',\n",
       " 'cranberries',\n",
       " 'dates',\n",
       " 'grapes',\n",
       " 'guava',\n",
       " 'kiwi fruit',\n",
       " 'melons',\n",
       " 'papaya',\n",
       " 'mango',\n",
       " 'feijoa',\n",
       " 'passionfruit',\n",
       " 'casaha melon',\n",
       " 'peaches',\n",
       " 'nectarines',\n",
       " 'plums',\n",
       " 'pears',\n",
       " 'sapote',\n",
       " 'pineapple',\n",
       " 'plantains',\n",
       " 'pomegranate',\n",
       " 'artichokes',\n",
       " 'whole',\n",
       " 'asparagus',\n",
       " 'bamboo shoots',\n",
       " 'beans and peas',\n",
       " 'beets',\n",
       " 'bok choy',\n",
       " 'broccoli and broccoli raab (rapini)',\n",
       " 'brussels sprouts',\n",
       " 'cabbage',\n",
       " 'carrots',\n",
       " 'parsnips',\n",
       " 'cauliflower',\n",
       " 'celery',\n",
       " 'corn on the cob',\n",
       " 'cucumbers',\n",
       " 'eggplant',\n",
       " 'ginger root',\n",
       " 'greens',\n",
       " 'leeks',\n",
       " 'lettuce',\n",
       " 'mushrooms',\n",
       " 'okra',\n",
       " 'onions',\n",
       " 'peppers',\n",
       " 'potatoes',\n",
       " 'pumpkins',\n",
       " 'radishes',\n",
       " 'rhubarb',\n",
       " 'rutabagas',\n",
       " 'squash',\n",
       " 'tamarind',\n",
       " 'taro',\n",
       " 'tomatoes',\n",
       " 'turnips',\n",
       " 'yuca cassava',\n",
       " 'dough',\n",
       " 'frozen potato products',\n",
       " 'frozen pretzels',\n",
       " 'fruits',\n",
       " 'ice cream',\n",
       " 'ice pops',\n",
       " 'juice concentrates',\n",
       " 'lobster tails',\n",
       " 'pancakes',\n",
       " 'waffles',\n",
       " 'sausages',\n",
       " 'sherbet',\n",
       " 'sorbet',\n",
       " 'shellfish',\n",
       " 'soy crumbles and hot dogs',\n",
       " 'soy meat substitutes',\n",
       " 'tempeh',\n",
       " 'frozen entrees',\n",
       " 'vegetables',\n",
       " 'fresh pasta',\n",
       " 'beans',\n",
       " 'lentils',\n",
       " 'pasta',\n",
       " 'dry egg noodles',\n",
       " 'peas',\n",
       " 'rice',\n",
       " 'barbecue sauce',\n",
       " 'chutney',\n",
       " 'cream sauces',\n",
       " 'milk solids',\n",
       " 'dry gravy mixes',\n",
       " 'gravy',\n",
       " 'honey',\n",
       " 'horseradish',\n",
       " 'jams',\n",
       " 'jellies',\n",
       " 'preserves',\n",
       " 'ketchup',\n",
       " 'cocktail',\n",
       " 'chili sauce',\n",
       " 'marinades',\n",
       " 'mayonnaise',\n",
       " 'mustard',\n",
       " 'pickles',\n",
       " 'pesto',\n",
       " 'salad dressings',\n",
       " 'salsa',\n",
       " 'sauce mixes',\n",
       " 'spaghetti sauce',\n",
       " 'soy sauce teriyaki sauce',\n",
       " 'vinegar',\n",
       " 'worcestershire sauce',\n",
       " 'jars pouches',\n",
       " 'dinners',\n",
       " 'cereal',\n",
       " 'dry mixes',\n",
       " 'formula',\n",
       " 'liquid concentrate ready-to-feed formula',\n",
       " 'applesauce',\n",
       " 'bacon bits',\n",
       " 'canned goods',\n",
       " 'chocolate syrup',\n",
       " 'crackers',\n",
       " 'graham cracker animal cracker',\n",
       " 'dried',\n",
       " 'gummy (fruit) snacks',\n",
       " 'marshmallows',\n",
       " 'marshmallow crème',\n",
       " 'molasses',\n",
       " 'nuts',\n",
       " 'peanut butter',\n",
       " 'pectin',\n",
       " 'popcorn',\n",
       " 'potato chips',\n",
       " 'pretzels',\n",
       " 'pudding mixes',\n",
       " 'soup mixes',\n",
       " 'sun dried tomatoes',\n",
       " 'syrup',\n",
       " 'toaster pastries',\n",
       " 'coffee',\n",
       " 'diet powder mixes and drink mixes',\n",
       " 'fruit juice in cartons',\n",
       " 'fruit drinks',\n",
       " 'punch',\n",
       " 'juice',\n",
       " 'boxes',\n",
       " 'nectar',\n",
       " 'soda',\n",
       " 'soy rice beverage',\n",
       " 'tea',\n",
       " 'water',\n",
       " 'kumquats',\n",
       " 'bagged greens',\n",
       " 'tahini',\n",
       " 'egg salad',\n",
       " 'potato salad',\n",
       " 'seafood salads',\n",
       " 'chicken salad',\n",
       " 'ham salad',\n",
       " 'pasta salad',\n",
       " 'soy milk',\n",
       " 'yams sweet potatoes',\n",
       " 'kale',\n",
       " 'quinoa',\n",
       " 'almond milk',\n",
       " 'rice milk',\n",
       " 'coconut milk',\n",
       " 'turkey bacon',\n",
       " 'fruit cocktail',\n",
       " 'black bean sauce',\n",
       " 'oyster sauce',\n",
       " 'hoisin sauce',\n",
       " 'pork roll',\n",
       " 'almonds',\n",
       " 'cashews',\n",
       " 'macadamias',\n",
       " 'peanuts',\n",
       " 'pecans',\n",
       " 'pistachios',\n",
       " 'walnuts',\n",
       " 'lime juice',\n",
       " 'lemon juice',\n",
       " 'bagel',\n",
       " 'muffin',\n",
       " 'coconut oil',\n",
       " 'orange juice',\n",
       " 'roasted red peppers',\n",
       " 'whole wheat flour',\n",
       " 'whole wheat bread',\n",
       " 'red wine',\n",
       " 'white wine',\n",
       " 'dry stuffing mix',\n",
       " 'powdered milk',\n",
       " 'almond butter',\n",
       " 'cashew butter',\n",
       " 'salt',\n",
       " 'black pepper',\n",
       " 'cajun seasoning blend',\n",
       " 'cinnamon',\n",
       " 'cumin',\n",
       " 'garlic powder',\n",
       " 'onion powder',\n",
       " 'nutmeg',\n",
       " 'nacho cheese',\n",
       " 'star fruit',\n",
       " 'prickly pear',\n",
       " 'pitaya dragon fruit',\n",
       " 'strawberries',\n",
       " 'raspberries',\n",
       " 'cherries',\n",
       " 'broth',\n",
       " 'beef broth stock consommé',\n",
       " 'chicken broth stock consommé',\n",
       " 'vegetable stock broth',\n",
       " 'ricotta',\n",
       " 'baby carrots',\n",
       " 'jicama',\n",
       " 'kimchi',\n",
       " 'kohlrabi',\n",
       " 'watermelon',\n",
       " 'cantaloupe',\n",
       " 'honeydew',\n",
       " 'refried beans',\n",
       " 'relish',\n",
       " 'tomato paste',\n",
       " 'tapenade',\n",
       " 'flaxseed',\n",
       " 'lemongrass',\n",
       " 'cilantro',\n",
       " 'mint',\n",
       " 'basil',\n",
       " 'oregano',\n",
       " 'rosemary',\n",
       " 'chives',\n",
       " 'thyme',\n",
       " 'salami',\n",
       " 'canadian bacon',\n",
       " 'canola oil',\n",
       " 'almond oil',\n",
       " 'sunflower oil',\n",
       " 'grapeseed oil',\n",
       " 'duck fat',\n",
       " 'bacon grease',\n",
       " 'frying oil',\n",
       " 'almond extract',\n",
       " 'cinnamon extract',\n",
       " 'lemon extract',\n",
       " 'pure vanilla extract',\n",
       " 'butter flavor',\n",
       " 'coconut flavor',\n",
       " '\"genuine\" maple syrup',\n",
       " 'apple juice',\n",
       " 'carrot juice',\n",
       " 'hard liquors',\n",
       " 'cream liquors',\n",
       " 'macaroons',\n",
       " 'string cheese',\n",
       " 'vegan cheddar cheese',\n",
       " 'quark',\n",
       " 'zucchini',\n",
       " 'hot peppers',\n",
       " 'bean sprouts',\n",
       " 'swiss chard',\n",
       " 'puff pastry',\n",
       " 'biscuits',\n",
       " 'pie crust',\n",
       " 'granola',\n",
       " 'pork rinds',\n",
       " 'apple cider vinegar',\n",
       " 'nutrition supplement drinks',\n",
       " 'hot sauce',\n",
       " 'thai red curry paste',\n",
       " 'yeast',\n",
       " 'cranberry sauce',\n",
       " 'vegetable juice',\n",
       " 'marinated vegetables',\n",
       " 'pizza',\n",
       " 'chia seeds',\n",
       " 'bread',\n",
       " 'amaranth',\n",
       " 'barley',\n",
       " 'buckwheat',\n",
       " 'farro',\n",
       " 'millet',\n",
       " 'oats',\n",
       " 'rye',\n",
       " 'sorghum',\n",
       " 'spelt',\n",
       " 'teff',\n",
       " 'rabbit',\n",
       " 'spaghetti squash',\n",
       " 'garam masala',\n",
       " 'tomato sauce',\n",
       " 'cherry tomatoes',\n",
       " 'coconut cream',\n",
       " 'coleslaw',\n",
       " 'pumpkin seeds',\n",
       " 'sunflower seeds',\n",
       " 'parsley',\n",
       " 'sesame oil',\n",
       " 'sesame seeds',\n",
       " 'capers',\n",
       " 'chocolate hazlenut spread',\n",
       " 'roasted nuts (peanuts',\n",
       " 'almonds)',\n",
       " 'tuna',\n",
       " 'seafood',\n",
       " 'pimento cheese',\n",
       " 'bison',\n",
       " 'salad dressing',\n",
       " 'grits',\n",
       " 'cheese curds',\n",
       " 'vegetable soup',\n",
       " 'chorizo',\n",
       " 'cinnamon rolls',\n",
       " 'cooking wine',\n",
       " 'bulgur',\n",
       " 'bratwurst',\n",
       " 'edamame',\n",
       " 'breadcrumbs',\n",
       " 'ghee',\n",
       " 'corn syrup',\n",
       " 'instant breakfast drinks',\n",
       " 'pine nuts',\n",
       " 'coconut flour',\n",
       " 'polenta',\n",
       " 'cereal granola bars',\n",
       " 'coconut water',\n",
       " 'celery root',\n",
       " 'apple cider',\n",
       " 'yuzu juice',\n",
       " 'yuzu',\n",
       " 'pastrami',\n",
       " 'kugel',\n",
       " 'avocado oil',\n",
       " 'balsamic vinegar',\n",
       " 'aioli',\n",
       " 'base',\n",
       " 'anchovies',\n",
       " 'radicchio',\n",
       " 'prosciutto',\n",
       " 'arugula',\n",
       " 'mung bean',\n",
       " 'croutons']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodKeeperInfo() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "347f3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "commonVerbs = getCommonVerbs(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5604f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the common verbs...\n",
      "Common Verbs gathered... \n",
      "\n",
      "['eat', 'know', 'like', 'think', 'want', 'got', 'love', 'use', 'need', 'add']\n",
      "⠋ ~~~~~~~~~~~~~~~~~0~~~~~~~~~~~~~~~~~\n",
      "getting from path:  output/model-last\n",
      "⠹ Starting training process Model loaded...\n",
      "⠏ Gathering training data... Total keywords:  15\n",
      "Total Tweets:  0\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~1~~~~~~~~~~~~~~~~~\n",
      "⠋ Starting training process  getting from path:  output/model-last\n",
      "⠹ Starting training process  Model loaded...\n",
      "⠧ Gathering training data... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/195 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                             \r",
      "\r",
      "⠇ sorting data for docBin file and training if needed on iteration: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 195/195 [00:00<00:00, 2506.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      \r",
      "\r",
      "⠏ sorting data for docBin file and training if needed on iteration: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ sorting data for docBin file and training if needed on iteration: 1 [i] Saving to output directory: output\n",
      "[i] Using GPU: 0\n",
      "\n",
      "Total keywords:  15\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt'] \n",
      "\n",
      "\n",
      "⠋ Done with current training on iteration: 1                          [[35  9]\n",
      " [ 2 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.80      0.86        44\n",
      "           0       0.80      0.95      0.87        39\n",
      "\n",
      "    accuracy                           0.87        83\n",
      "   macro avg       0.88      0.87      0.87        83\n",
      "weighted avg       0.88      0.87      0.87        83\n",
      "\n",
      "Total keywords:  195\n",
      "Total Tweets:  195\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~2~~~~~~~~~~~~~~~~~\n",
      "⠙ Starting training processgetting from path:  output/model-last      \n",
      "⠸ Starting training process                                           Model loaded...\n",
      "⠙ Gathering training data...                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3751 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠹ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 423/3751 [00:00<00:01, 2141.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠸ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 981/3751 [00:00<00:01, 2576.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠼ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 1326/3751 [00:00<00:00, 2880.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠴ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▍     | 1680/3751 [00:00<00:00, 3097.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠦ sorting data for docBin file and training if needed on iteration: 2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 2427/3751 [00:00<00:00, 3393.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠧ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 2835/3751 [00:00<00:00, 3596.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠇ sorting data for docBin file and training if needed on iteration: 2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 3247/3751 [00:01<00:00, 3747.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 3637/3751 [00:01<00:00, 3789.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      \r",
      "\r",
      "⠏ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3751/3751 [00:01<00:00, 3283.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠋ sorting data for docBin file and training if needed on iteration: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ sorting data for docBin file and training if needed on iteration: 2 [i] Saving to output directory: output\n",
      "[i] Using GPU: 0\n",
      "\n",
      "Total keywords:  195\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail'] \n",
      "\n",
      "\n",
      "⠦ Done with current training on iteration: 2                          [[35  9]\n",
      " [ 2 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.80      0.86        44\n",
      "           0       0.80      0.95      0.87        39\n",
      "\n",
      "    accuracy                           0.87        83\n",
      "   macro avg       0.88      0.87      0.87        83\n",
      "weighted avg       0.88      0.87      0.87        83\n",
      "\n",
      "Total keywords:  412\n",
      "Total Tweets:  3751\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~3~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Starting training process                                           getting from path:  output/model-last\n",
      "⠏ Starting training process                                           Model loaded...\n",
      "⠧ Gathering training data...                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3757 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠇ sorting data for docBin file and training if needed on iteration: 3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 480/3757 [00:00<00:01, 2415.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 778/3757 [00:00<00:01, 2670.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      \r",
      "\r",
      "⠏ sorting data for docBin file and training if needed on iteration: 3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 1142/3757 [00:00<00:00, 3048.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠋ sorting data for docBin file and training if needed on iteration: 3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1821/3757 [00:00<00:00, 3253.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠙ sorting data for docBin file and training if needed on iteration: 3Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 2623/3757 [00:00<00:00, 3667.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠹ sorting data for docBin file and training if needed on iteration: 3 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 3427/3757 [00:01<00:00, 3826.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠸ sorting data for docBin file and training if needed on iteration: 3 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3757/3757 [00:01<00:00, 3454.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      \r",
      "\r",
      "⠼ sorting data for docBin file and training if needed on iteration: 3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ sorting data for docBin file and training if needed on iteration: 3 [i] Saving to output directory: output\n",
      "[i] Using GPU: 0\n",
      "\n",
      "Total keywords:  412\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root'] \n",
      "\n",
      "\n",
      "⠇ Done with current training on iteration: 3                          Decreasing entityCheckCount variable by 1\n",
      "entityCheckCount =  2\n",
      "⠦ Done with current training on iteration: 3                          [[35  9]\n",
      " [ 2 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.80      0.86        44\n",
      "           0       0.80      0.95      0.87        39\n",
      "\n",
      "    accuracy                           0.87        83\n",
      "   macro avg       0.88      0.87      0.87        83\n",
      "weighted avg       0.88      0.87      0.87        83\n",
      "\n",
      "Total keywords:  412\n",
      "Total Tweets:  3757\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~4~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Starting training process                                           getting from path:  output/model-last\n",
      "⠏ Starting training process                                           Model loaded...\n",
      "⠇ Gathering training data...                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠏ sorting data for docBin file and training if needed on iteration: 4Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 234/6843 [00:00<00:02, 2319.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠋ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 501/6843 [00:00<00:02, 2512.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠙ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 1117/6843 [00:00<00:01, 2918.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠹ sorting data for docBin file and training if needed on iteration: 4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 1497/6843 [00:00<00:01, 3229.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠸ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2284/6843 [00:00<00:01, 3610.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠼ sorting data for docBin file and training if needed on iteration: 4Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 2695/6843 [00:00<00:01, 3757.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠴ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 3086/6843 [00:00<00:00, 3800.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠦ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3919/6843 [00:01<00:00, 3979.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠧ sorting data for docBin file and training if needed on iteration: 4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 4374/6843 [00:01<00:00, 4150.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠇ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 5254/6843 [00:01<00:00, 4253.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠏ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5680/6843 [00:01<00:00, 4115.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 6093/6843 [00:01<00:00, 4111.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠋ sorting data for docBin file and training if needed on iteration: 4 \n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 6505/6843 [00:01<00:00, 4089.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6843/6843 [00:01<00:00, 3812.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      \r",
      "\r",
      "⠙ sorting data for docBin file and training if needed on iteration: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ sorting data for docBin file and training if needed on iteration: 4 [i] Saving to output directory: output\n",
      "[i] Using GPU: 0\n",
      "\n",
      "Total keywords:  412\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root'] \n",
      "\n",
      "\n",
      "⠼ Done with current training on iteration: 4                          [[35  9]\n",
      " [ 2 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.80      0.86        44\n",
      "           0       0.80      0.95      0.87        39\n",
      "\n",
      "    accuracy                           0.87        83\n",
      "   macro avg       0.88      0.87      0.87        83\n",
      "weighted avg       0.88      0.87      0.87        83\n",
      "\n",
      "Total keywords:  419\n",
      "Total Tweets:  6843\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root', 'coffee creamer', 'egg substitutes', 'fruit cake', 'seasoning blends', 'tapiocas', 'dry egg noodles', 'vegan cheddar cheese'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~5~~~~~~~~~~~~~~~~~\n",
      "⠴ Starting training processgetting from path:  output/model-last      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠧ Starting training process                                           Model loaded...\n",
      "⠙ Gathering training data...                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠹ sorting data for docBin file and training if needed on iteration: 5Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 201/6843 [00:00<00:03, 1999.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠸ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 810/6843 [00:00<00:02, 2813.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠼ sorting data for docBin file and training if needed on iteration: 5Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1151/6843 [00:00<00:01, 3039.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠴ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 1528/6843 [00:00<00:01, 3298.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠦ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2279/6843 [00:00<00:01, 3559.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠧ sorting data for docBin file and training if needed on iteration: 5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▊      | 2636/6843 [00:00<00:01, 3556.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠇ sorting data for docBin file and training if needed on iteration: 5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▍     | 3058/6843 [00:00<00:01, 3759.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠏ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3893/6843 [00:01<00:00, 3953.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠋ sorting data for docBin file and training if needed on iteration: 5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 4289/6843 [00:01<00:00, 3896.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠙ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 5158/6843 [00:01<00:00, 4136.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "⠹ sorting data for docBin file and training if needed on iteration: 5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 5588/6843 [00:01<00:00, 4181.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠸ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 6027/6843 [00:01<00:00, 4238.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠼ sorting data for docBin file and training if needed on iteration: 5Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 6451/6843 [00:01<00:00, 3988.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "\r",
      "                                                                      \r",
      "\r",
      "⠴ sorting data for docBin file and training if needed on iteration: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6843/6843 [00:01<00:00, 3736.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ sorting data for docBin file and training if needed on iteration: 5 [i] Saving to output directory: output\n",
      "[i] Using GPU: 0\n",
      "\n",
      "Total keywords:  419\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root', 'coffee creamer', 'egg substitutes', 'fruit cake', 'seasoning blends', 'tapiocas', 'dry egg noodles', 'vegan cheddar cheese'] \n",
      "\n",
      "\n",
      "⠴ Done with current training on iteration: 5                          Decreasing entityCheckCount variable by 1\n",
      "entityCheckCount =  1\n",
      "⠴ Done with current training on iteration: 5                          [[35  9]\n",
      " [ 2 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.80      0.86        44\n",
      "           0       0.80      0.95      0.87        39\n",
      "\n",
      "    accuracy                           0.87        83\n",
      "   macro avg       0.88      0.87      0.87        83\n",
      "weighted avg       0.88      0.87      0.87        83\n",
      "\n",
      "Total keywords:  419\n",
      "Total Tweets:  6843\n",
      "List of Keywords:\n",
      "\n",
      " ['cheese', 'chicken', 'milk', 'butter', 'juice', 'cream', 'fruit', 'rice', 'water', 'bread', 'garlic', 'sugar', 'chocolate', 'fish', 'salt', 'yogurt', 'almond milk', 'greens', 'granola', 'nuts', 'chia seeds', 'whole', 'onions', 'apples', 'peanut butter', 'peaches', 'squash', 'soda', 'flour', 'garlic powder', 'buttermilk', 'potatoes', 'carrots', 'cottage cheese', 'sour cream', 'turkey', 'beans', 'cream cheese', 'bagel', 'pretzels', 'celery', 'white wine', 'fruit cocktail', 'crackers', 'whipped cream', 'margarine', 'ice cream', 'pineapple', 'whipped topping', 'mango', 'sausage', 'pork', 'onion powder', 'dried', 'parsley', 'eggs', 'beef', 'vegetables', 'rotisserie chicken', 'hot dogs', 'dips', 'meats', 'tuna', 'frozen entrees', 'soup', 'bacon', 'pasta', 'pizza', 'fried chicken', 'potato chips', 'cut', 'lean fish', 'cauliflower', 'cooked fish', 'honey', 'stuffed', 'dates', 'scallops', 'yuzu', 'shrimp', 'corn syrup', 'baking soda', 'molasses', 'yeast', 'cornmeal', 'vinegar', 'worcestershire sauce', 'mushrooms', 'broth', 'peppers', 'gravy', 'oats', 'guava', 'coconut', 'cake', 'lemon juice', 'strawberries', 'cheesecake', 'chicken nuggets', 'cinnamon', 'muffin', 'danish', 'pies', 'tomato sauce', 'apple juice', 'pickles', 'coffee', 'goat', 'baking powder', 'sesame seeds', 'pancakes', 'raspberries', 'cornstarch', 'hot sauce', 'grits', 'chives', 'tamarind paste', 'lime juice', 'tomatoes', 'cumin', 'chili powder', 'black pepper', 'pie crust', 'cooked rice', 'patties', 'nutmeg', 'peas', 'refried beans', 'bananas', 'okra', 'cabbage', 'syrup', 'juice concentrates', 'ketchup', 'basil', 'lentils', 'fruits', 'pesto', 'soy milk', 'hoisin sauce', 'sesame oil', 'chili sauce', 'peanuts', 'ham', 'mustard', 'bacon bits', 'chutney', 'sun dried tomatoes', 'boxes', 'string cheese', 'punch', 'waffles', 'crab', 'pasta salad', 'cereal', 'grapes', 'nectarines', 'pears', 'cashews', 'tea', 'pudding', 'coconut cream', 'coconut water', 'cocktail', 'walnuts', 'dough', 'roasted red peppers', 'whole wheat bread', 'thyme', 'mayonnaise', 'berries', 'coleslaw', 'tomato paste', 'oregano', 'canola oil', 'mung bean', 'ghee', 'avocado oil', 'almond oil', 'grapeseed oil', 'spelt', 'coconut milk', 'pure vanilla extract', 'butter flavor', 'olives', 'salsa', 'puff pastry', 'biscuits', 'orange juice', 'vegetable soup', 'garam masala', 'eggplant', 'pine nuts', 'coconut oil', 'quail', 'almonds', 'oysters', 'pate', 'lettuce', 'oils', 'avocados', 'pumpkin seeds', 'flaxseed', 'fatty fish', 'herring', 'quiche', 'chorizo', 'aioli', 'blueberries', 'ricotta', 'capers', 'base', 'herbs', 'kale', 'cilantro', 'asparagus', 'eggnog', 'tofu', 'egg dishes', 'quinoa', 'kimchi', 'kefir', 'apple cider vinegar', 'jellies', 'tortillas', 'apricots', 'gelatin', 'marshmallows', 'cookie dough', 'cherries', 'mint', 'lamb', 'arugula', 'pistachios', 'beets', 'plantains', 'veal', 'bratwurst', 'sausages', 'shellfish', 'pork roll', 'venison', 'leeks', 'pheasant', 'seafood', 'marinades', 'rabbit', 'guacamole', 'corned beef', 'pastrami', 'chicken salad', 'rye', 'jerky', 'meat products', 'dinners', 'goose', 'applesauce', 'chicken parts', 'oyster sauce', 'cranberries', 'capon', 'duckling', 'horseradish', 'cornish hens', 'giblets', 'turducken', 'pastries', 'canned goods', 'rosemary', 'cucumbers', 'potato salad', 'salami', 'canned chicken', 'fruit drinks', 'powdered milk', 'red wine', 'salad dressings', 'sunflower seeds', 'caviar', 'croutons', 'breadcrumbs', 'popcorn', 'crayfish', 'mussels', 'crab meat', 'crab legs', 'lobster tails', 'zucchini', 'squid', 'anchovies', 'cherry tomatoes', 'miso', 'cookies', 'turnips', 'tempeh', 'hot peppers', 'barley', 'soy flour', 'whole wheat flour', 'leftovers', 'cooked pasta', 'farro', 'brownie', 'melons', 'edamame', 'hummus', 'pecans', 'balsamic vinegar', 'stews', 'casseroles', 'passionfruit', 'doughnuts', 'citrus fruit', 'watermelon', 'polenta', 'cream pies', 'fruit pies', 'egg salad', 'buckwheat', 'coconut flour', 'lemongrass', 'shortening', 'tamarind', 'sorbet', 'nectar', 'duck fat', 'cantaloupe', 'sugar substitutes', 'plums', 'papaya', 'pomegranate', 'kumquats', 'pumpkins', 'artichokes', 'preserves', 'cherimoya', 'kiwi fruit', 'coconuts', 'star fruit', 'tapenade', 'bacon grease', 'parsnips', 'bamboo shoots', 'brussels sprouts', 'millet', 'beans and peas', 'radishes', 'swiss chard', 'rhubarb', 'almonds)', 'bok choy', 'jicama', 'radicchio', 'bean sprouts', 'kohlrabi', 'pork rinds', 'baby carrots', 'relish', 'ginger root', 'tahini', 'prosciutto', 'rutabagas', 'cranberry sauce', 'taro', 'carrot juice', 'ice pops', 'sherbet', 'barbecue sauce', 'jams', 'cheese curds', 'sauce mixes', 'spaghetti sauce', 'apple cider', 'formula', 'chocolate syrup', 'pectin', 'toaster pastries', 'seafood salads', 'cinnamon rolls', 'ham salad', 'rice milk', 'amaranth', 'turkey bacon', 'black bean sauce', 'macadamias', 'salad dressing', 'almond butter', 'cashew butter', 'nacho cheese', 'prickly pear', 'honeydew', 'quark', 'canadian bacon', 'sunflower oil', 'bison', 'almond extract', 'milk solids', 'lemon extract', 'yuzu juice', 'coconut flavor', 'hard liquors', 'macaroons', 'spaghetti squash', 'vegetable juice', 'marinated vegetables', 'bulgur', 'sorghum', 'teff', 'fresh pasta', 'pimento cheese', 'cooking wine', 'celery root', 'coffee creamer', 'egg substitutes', 'fruit cake', 'seasoning blends', 'tapiocas', 'dry egg noodles', 'vegan cheddar cheese'] \n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~6~~~~~~~~~~~~~~~~~\n",
      "⠦ Starting training processgetting from path:  output/model-last      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠇ Starting training process                                           Model loaded...\n",
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5644\\1266341519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcommonVerbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommonVerbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5644\\4269212709.py\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(data, commonVerbs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                         \u001b[1;31m# checks the rank with the model it loaded (previous one or the current one that is being trained?)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mrankTweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mentityCheckCount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                             \u001b[0mcheckTweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5644\\100953457.py\u001b[0m in \u001b[0;36mrankTweet\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#update rank tweet to take the counter as a parameter and condense both rankings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrankTweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m#     tweetKeywords = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m     nlp = load_model_from_config(\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;31m# registry, including custom subclasses provided via entry points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mlang_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lang\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     nlp = lang_cls.from_config(\n\u001b[0m\u001b[0;32m    529\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, vocab, disable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1777\u001b[0m         \u001b[1;31m# then we would load them twice at runtime: once when we make from config,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1778\u001b[0m         \u001b[1;31m# and then again when we load from disk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1779\u001b[1;33m         \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlang_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1780\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mafter_creation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m             \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mafter_creation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mtokenizer_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tokenizer\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"nlp\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tokenizer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mcreate_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer_cfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tokenizer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_error_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mtokenizer_factory\u001b[1;34m(nlp)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0msuffix_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_suffix_regex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msuffixes\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0minfix_finditer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_infix_regex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfixes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minfixes\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         return Tokenizer(\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mrules\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\_dict_proxies.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, doc, items)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Doc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpanGroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     ) -> None:\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mUserDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainModel(data=training_data,commonVerbs=commonVerbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccecec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommonVerbs():\n",
    "    import en_core_web_lg\n",
    "    nlp2 = spacy.load(\"en_core_web_lg\")\n",
    "    count = 0\n",
    "    myVerbs = {}\n",
    "    for i in range(10): #(len(training_data)):\n",
    "        doc = nlp2(training_data[0][i])\n",
    "        for token in doc:\n",
    "            print(token, token.pos_)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                if token.text in myVerbs:\n",
    "                    myVerbs[token.text] = myVerbs[token.text] + 1\n",
    "                else:\n",
    "                    if token.text not in nlp2.Defaults.stop_words:\n",
    "                        myVerbs[token.text] = 1\n",
    "        \n",
    "    print(count)\n",
    "    topVerbs = dict(sorted(myVerbs.items(), key = lambda item: item[1], reverse=True)[:10])\n",
    "    return [key for key in topVerbs]\n",
    "# print(getCommonVerbs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(live_tweets[5][:10])\n",
    "# print(test_data['tweet'][5])\n",
    "print(\"Trying the model indidvidually \")\n",
    "try:\n",
    "    nlp = spacy.load(MODEL_PATH)\n",
    "    print('Model loaded...')\n",
    "except:\n",
    "    print('No model...')\n",
    "    \n",
    "x = nlp(preProcess(\"he does like to eat cream cheese\"))\n",
    "if x.doc.ents:\n",
    "    ent_recognize(preProcess(\"he does like to eat cream cheese\"))\n",
    "    for i in x.doc.ents:\n",
    "        print(i)\n",
    "else:\n",
    "    print(\"nothing found...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdb35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def findPreviousNextWord(tweets, keywords):\n",
    "foodKeeperKeywords = foodKeeperInfo()\n",
    "commonWords = {}\n",
    "commonPairWords = {}\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "for i in range(len(training_data)):\n",
    "    #Preprocess and remove stop words from each tweet\n",
    "    myTweet = preProcess(training_data[0][i]).split()\n",
    "    myTweet = [word for word in myTweet if not word in nlp2.Defaults.stop_words]\n",
    "    \n",
    "    for i in range(len(myTweet)):\n",
    "        if myTweet[i] in foodKeeperKeywords:\n",
    "            try:\n",
    "                leftWord = myTweet[i-1]\n",
    "                if leftWord in commonWords:\n",
    "                    commonWords[leftWord] += 1\n",
    "                else:\n",
    "                    commonWords[leftWord] = 1\n",
    "            except:\n",
    "                pass\n",
    "            try: \n",
    "                rightWord = myTweet[i+1]\n",
    "                if rightWord in commonWords:\n",
    "                    commonWords[rightWord] += 1\n",
    "                else:\n",
    "                    commonWords[rightWord] = 1\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if (leftWord, rightWord) in commonPairWords:\n",
    "                    commonPairWords[(leftWord,rightWord)] += 1\n",
    "                else:\n",
    "                    commonPairWords[(leftWord,rightWord)] = 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#commonWords are words that occur before and after a keyword\n",
    "commonWords = dict(sorted(commonWords.items(), key = lambda item: item[1], reverse=True))\n",
    "commonWords = [key for key in commonWords]\n",
    "\n",
    "#commonPairWords are pairs or words that occur before and after a keyword\n",
    "commonPairWords = dict(sorted(commonPairWords.items(), key = lambda item: item[1], reverse=True))\n",
    "# print(training_data[0][1000])\n",
    "# print(foodKeeperKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#count all words and adjacent\n",
    "def findAdjacentWords(data,keywords):\n",
    "    allWords = {}\n",
    "    count = 0\n",
    "    for tweet in data[0][:10]:\n",
    "        splitTweet = preProcess(tweet).split()\n",
    "        for i in range(len(splitTweet)):\n",
    "            word = splitTweet[i]\n",
    "            try:\n",
    "                biword = word + \" \" + splitTweet[i + 1]\n",
    "                if biword in keywords:\n",
    "                    print(\"Biword:\", biword)\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                triword = word + \" \" + splitTweet[i + 1] + \" \" + splitTweet[i + 2]\n",
    "                if triword in keywords:\n",
    "                    print(\"Triword:\", triword)\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "    \n",
    "    '''mySent = \"I am eating chicken for breakfast lunch and dinner tonight.\"\n",
    "    mySearch = re.search('chicken', mySent)\n",
    "    mySpan = mySearch.span()\n",
    "    print(mySent[mySpan[0]:mySpan[1]])\n",
    "    '''\n",
    "# findAdjacentWords(training_data, foodKeeperKeywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
